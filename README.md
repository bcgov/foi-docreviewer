# FOI Dedupe, Document Extraction ,  NLP Services and related Data, Document Storage

## Introduction
FOI Dedupe, Document extraction and NLP Services described in this documentation will be a value added service(s) on top of current FOI FLOW implementation. These services will support FOI FLOW application for document review, harms assessment and Redaction(PII, Confidentials etc.).  Functionally various personas on FOI FLOW app like IAO Analysts, Ministry level FOI Coordinators can ease their effort while doing document deduplication, document review and redaction on digital records having PII and other confidential data. 

## Architecture and Design
### Architecture
![Here is the Full FOI Flow architecture](./archanddesign/images/Option1_TechArch_Emerald.jpg)

The above architecture diagram describes full architecture of FOI FLOW ( on left - OCP Silver) , with the new Dedupe , NLP Services which will be deployed on BC GOV Emerald Cluster (on Right). For the purpose of elaborating Dedupe and related services along with the data storage , please follow below subsection of the above diagram.
![image](https://user-images.githubusercontent.com/78570775/204931018-582b9630-b2be-44ed-9ef2-56d6625668c1.png)

#### Component and module wise description
**EventHub** : This component or tool will be used for purpose of storing persistent event message stream from various Producer components(aka Publisher!) and trigger its corresponding Consumer components(aka Subscriber!) to do related task in a disconnected - loosely coupled manner. All the consumer components will have their "STREAM TOPIC/KEY" to listen to their channel of messages from Producer. For instance, we have "Records Uploads" component on our FOI FLOW app, which will be a PRODUCER for our "Dedupe Service/Engine" to CONSUME messages for its actions next. Based on our analysis , we decided to go ahead with [REDIS STREAMS](https://redis.io/docs/data-types/streams-tutorial/). Apache Kafka was our ideal choice, but decided to go with REDIS STREAM due to certain technical reasons. This component has no Protected C data stored, but only few metadata on records like Document Path, Identifiers, Minisitry codes etc. Hence this will be in a LOW security data zone/tag in EMERALD 

**Dedupe Services / Engine** : This will be a python based custom application or computing micro-services, which has shared datastore with other components like REDACTION API. This component will be CONSUMER and PRODUCER at the same time. The computing actions on this component is triggered by REDIS listener events. Based on the message data with S3(ObjectStorage) File Path, document will transited to this microservices as binary stream , which is then hashed on two criterias #1) Document Metadata #2) Document Content. The document hash is stored inside datastore , POSTGRES DB. There will be another supporting sub-conversion services to convert MS Office files to PDF. In the end, once the DEDUPE for a document is over, it will then PRODUCE a message for DOCUMENT EXTRACT SERVICE.  This component handles all types of documents(Prot. A, B, C) its metadata and actual content , also stores its hash in the datastore. This service including its datastore need to be under HIGH security data zone/tag. Network Communication for this component is limited to REDIS STREAM(EventHub) and its BACK END Datastore.

**Document Processor - Extraction Service** : The actions on this compoenent is dependent on message stream PRODUCED from above mentioned DEDUPE SERVICES. This  component will extract the document content in various formats - initially with PDF! and make it easy(STEMMING, SORTING by content length, sections etc) for the next Service - NLP Engine. The triggering action(CONSUMER) for this component is based on the PRODUCER message from above mentioned DEDUPE service. We will python based libraies to extract content all kinds of  PDF - Structured and UnStructured, this will include usuage of libraries PDFMiner, OCR - Pytessaract etc. Once the 'Document extract process' is over on a given document,it will create a PRODUCER message for the next service - NLP . This component will handles all types of documents(Prot. A, B, C) its metadata and actual content , also stores its content in the datastore - POSTGRES or Any NoSQL DB. This service , including its datastore need to be under HIGH security data zone/tag. Network Communication for this component is limited to REDIS STREAM(EventHub) and its BACK END Datastore.

**NLP Processor - AI Component** : The actions on the component is dependent on the message stream PRODUCED messages from above metioned Document Processor and this is last component in the chain to CONSUME messages from REDIS STREAM. The purpose of this component is to process the extracted data from the document, which is in different levels, sub sections etc and understand the occurence of PII and other confidential data. The idenitified data, to be specific on AI language 'Named Entity Recognition' will happen using the python libraries like spacy,NLTK and some cases we will use Facebook's Duckling. Once 'Named Entities' has been identified , it is then stored into its dedicated datastore by DOCUMENT IDENTIFIER, PAGE NUMBER , SECTION IDENTIFIER etc. This service , including its datastore need to be under HIGH security data zone/tag. Network Communication for this component is limited to REDIS STREAM(EventHub) and its BACK END Datastore.

** API Component ** : API Component is basically a secured HTTPS REST Endpoint which talks with  FRONT END React Apps like REDACTION App and existing FOI FLOW App. The one endpoint on the API component will status(GET) out various stages of document flow, like 'DEDUPE IN PROGRESS', 'DUPLICATE DOCUMENT','EXTRACTION IN PROGRESS' etc. This endpoint will be consumed by FOI FLOW FE app. The second endpoint of this API will cater mainly document data related tasks - Like GET-ing 'PII' and/or 'Named Entities' on each document. Also, second endpoint will have POST functionalities for REDACTION Web app to save 'ANNOTATIONS' , 'COMMENTS','REDACTIONS' etc. These informations are saved into a HIGH security tagged Datastore.

### Design
